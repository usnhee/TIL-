{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPaI1Rw3oKA+JOUS/krzFxa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/usnhee/TIL-/blob/master/function_T5_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUNf_rSvrnFp"
      },
      "outputs": [],
      "source": [
        " !git clone https://github.com/monologg/KoBERT-Transformers.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/')"
      ],
      "metadata": {
        "id": "M_D_owm4rvMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tokenization_kobert"
      ],
      "metadata": {
        "id": "Pn92fjMCr2zT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenization_kobert import KoBertTokenizer"
      ],
      "metadata": {
        "id": "iPZlfRlyr3R3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_t = KoBertTokenizer.from_pretrained('monologg/kobert')"
      ],
      "metadata": {
        "id": "Zj4M0ua5r96U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow #텐서플로우 설치\n",
        "!pip install sklearn #사이킷런 설치\n",
        "!pip install konlpy"
      ],
      "metadata": {
        "id": "peVjkD1hsCSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics.pairwise import manhattan_distances\n",
        "manhattan_d = manhattan_distances(tfidf_normalized[0:1],tfidf_normalized[1:2])"
      ],
      "metadata": {
        "id": "FSoLBiUUsP2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosin_sim(t_list, question):\n",
        "  t_list['score']=0.0\n",
        "  t_list.reset_index(drop=True, inplace=True)\n",
        "  for i in range(len(t_list)):\n",
        "      sentences = (t_list['context'][i], question)\n",
        "      tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n",
        "      cos_similar = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
        "      t_list['score'][i] = cos_similar[0][0]\n",
        "  # return cos_similar[0][0], type(cos_similar[0][0])\n",
        "  return t_list.loc[t_list['score'].idxmax()]['context']"
      ],
      "metadata": {
        "id": "cgf-IKO0sQfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cos_sim(A, B):\n",
        "  return dot(A, B)/(norm(A)*norm(B))"
      ],
      "metadata": {
        "id": "SXzsjQuEsS5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def in_title(question, dat):\n",
        "  dat = dat.drop_duplicates(['context'], ignore_index = True)\n",
        "  t_list = []\n",
        "  con_list = []\n",
        "  for i in range(len(dat)):\n",
        "    if dat['title'][i].split('(')[0] in question.replace(' ', ''):\n",
        "      contet = dat.loc[i,:]\n",
        "      t_list.append(contet)\n",
        "\n",
        "    else:\n",
        "      pass\n",
        "  t_list = pd.DataFrame(t_list, columns = dat.columns)\n",
        "  con_list = dat[dat['context'].str.contains(okt.nouns(question)[0])]\n",
        "  use = pd.concat([t_list, con_list], ignore_index=True)\n",
        "  if len(use) >0:\n",
        "    return use\n",
        "  else :\n",
        "    return print('데이터 없음')\n"
      ],
      "metadata": {
        "id": "e3HNkiQFstBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def title_re(t_list, question):     \n",
        "\n",
        "  t_list['score']=0.0\n",
        "  t_list.reset_index(drop=True, inplace=True)\n",
        "  for i in range(len(t_list)):\n",
        "      sentences = (t_list['question'][i], question)\n",
        "      tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n",
        "      tfidf_normalized = tfidf_matrix/np.sum(tfidf_matrix)\n",
        "      manhattan_d = manhattan_distances(tfidf_normalized[0:1],tfidf_normalized[1:2])\n",
        "      t_list['score'][i] = manhattan_d[0][0]\n",
        "  t_list = t_list[t_list['score'] == min(t_list['score'])]\n",
        "  return t_list\n"
      ],
      "metadata": {
        "id": "abSgAK-JstMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def anw_test(context, question):\n",
        "\n",
        "  source_encoding=tokenizer(\n",
        "    question,\n",
        "    context,\n",
        "    max_length = 396,\n",
        "    padding=\"max_length\",\n",
        "    truncation=\"only_second\",\n",
        "    return_attention_mask=True,\n",
        "    add_special_tokens=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "  generated_ids = trained_model.model.generate(\n",
        "    input_ids=source_encoding[\"input_ids\"],\n",
        "    attention_mask=source_encoding[\"attention_mask\"],\n",
        "    num_beams=1,  # greedy search\n",
        "    max_length=80,\n",
        "    repetition_penalty=2.5,\n",
        "    early_stopping=True,\n",
        "    use_cache=True)\n",
        "  preds = [\n",
        "        tokenizer.decode(generated_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "        for generated_id in generated_ids]\n",
        "  return \"\".join(preds), context "
      ],
      "metadata": {
        "id": "6gk-IWFcstPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def last(question, dat):\n",
        "  dat = dat.drop_duplicates(['context'], ignore_index = True)\n",
        "  a = in_title(question,dat)   ## 타이틀, 컨텍스트 \n",
        "  b= title_re(a, question)    ## 질문,타이틀 유사도 \n",
        "  context =cosin_sim(b, question)    \n",
        "  answer = anw_test(context, question)   ## 정답 예측, 컨텍스트 \n",
        "  return answer"
      ],
      "metadata": {
        "id": "FScixs5WtKxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 예측 정답과 실제 정답 비교용 데이터프레임 생성 \n"
      ],
      "metadata": {
        "id": "-Qivhar3tqvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_df = dat.sample(n = 100 , random_state=15)\n",
        "re_sample = sample_df.reset_index()\n",
        "ans_list = []\n",
        "context_list = []\n",
        "\n",
        "for i in range(100): \n",
        "  try:\n",
        "    answer = last(re_sample.loc[i,'question'], re_sample)\n",
        "    ans_list.append(answer)\n",
        "    # for i in answer: \n",
        "    #   ans_list.append(a)\n",
        "    #   i+=1\n",
        "    #   context_list.append(a)\n",
        "  except:\n",
        "    ans_list.append('NaN')\n",
        "    context_list.append('NaN')"
      ],
      "metadata": {
        "id": "zXV6Cq2estRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df = re_sample.copy()\n",
        "for i in ans_list: \n",
        "  new_df['pred_ans'] = ans_list\n",
        "\n",
        "new_df.drop(['answer_start','answer_end'], axis=1)"
      ],
      "metadata": {
        "id": "5QtBoLhatZzD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}