{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensor_basic2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNG2oL/6GUYiUyTOX+QUjW/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/usnhee/TIL-/blob/master/tensor_basic2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8M2c4Ko3dg-P"
      },
      "outputs": [],
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense( 32,activation='relu', input_shape=(784,)))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "input_tensor = layers.Input(shape=(784,))\n",
        "x = layers.Dense(32, activation='relu')(input_tensor)\n",
        "output_tensor = layers.Dense(10, activation= 'softmax')(x)\n",
        "\n",
        "model = models.Model(inputs=input_tensor, outputs=output_tensor)"
      ],
      "metadata": {
        "id": "epcE2TRreEgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Keras \n",
        "- 모델 구조가 정의된 후에는 Sequential 모델을 사용했는지 함수형 API를 사용했는지 상관 없음. \n",
        "- 컴파일 단계에서 학습관정이 설정 -> 모델이 사용할 옵티마이저와 손실함수, 훈련하는 동안 모니터링하기 위해 필요한 측정지표를 지정 \n"
      ],
      "metadata": {
        "id": "uxlDwielhge9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import optimizers \n",
        "from tensorflow.keras import optimizers\n",
        "model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss='mse', metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLbB2VGohy1Z",
        "outputId": "1774b845-4f6f-487b-b2d4-048b7006dd46"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(input_tensor, target_tensor, batch_size=128, epochs=10)"
      ],
      "metadata": {
        "id": "5YtNndC-iTWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 영화 리뷰 분류- 이진분류\n",
        "- Positive / Negative 로 분류 \n",
        "- IMDB 데이터셋 (전처리 되어있음. 케라스)\n",
        "- 훈련데이터 25,000개, 테스트데이터 25,000개 \n",
        "- positive: negative (50:50)\n",
        "-  각 리뷰(단어 시퀀스)가 숫자 시퀀스로 변환되어 있음. -> 각 숫자는 사전에 있는 고유한 단어 \n",
        "- num_words=10000 매개변수는 훈련데이터에서 가장 자주 나타나는 단어 1만개만 사용하겠다는 의미"
      ],
      "metadata": {
        "id": "X5HWvLS9iq4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import imdb\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRwSYo48jdIh",
        "outputId": "1218c818-8e90-411b-c554-02215e17144a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLAi82mUjypp",
        "outputId": "f0f782d9-db4b-48b3-9050-707e9bf5e356"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_labels[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qse-raNpj3Th",
        "outputId": "531b432c-5895-41e2-c309-8d6e38a56058"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(max([max(sequence) for sequence in train_data]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_wqNRxFj6o-",
        "outputId": "29e2a32c-1850-43eb-d49c-9c8050f8947f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = dict([value,key] for (key, value) in word_index.items())\n",
        "decoded_review = ' '.join([reverse_word_index.get(i-3, '?') for i in train_data[0]])\n",
        "print(decoded_review)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qkoi4m2ckDdB",
        "outputId": "357965d3-fa19-4bfc-9aaa-f2e8823f45b4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n",
            "1654784/1641221 [==============================] - 0s 0us/step\n",
            "? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 신경망에 숫자리스트를 주입 불가 -> 리스트를 텐서로 만들기 \n",
        "  - 같은 길이가 되도록 리스트에 패딩(padding)을 추가하고 (samples, sequence_length) 크기의 정수 텐서로 변환 -> 정수 텐서를 다룰 수 있는 층(embedding)을 신경망의 첫번째 층으로 사용 \n",
        "  - 리스트를 원-핫 인코딩(one-hot encoding) 하여 0과 1의 벡터로 변환 -> 부동 소수 벡터데이터를 다룰 수 있는 Dense층을 신경망의 첫번째 층으로 사용 "
      ],
      "metadata": {
        "id": "R0evt2hwk02L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "\n",
        "def vectorize_sequences(seqs, dim=10000):\n",
        "  results = np.zeros((len(seqs), dim))\n",
        "  for i, seq in enumerate(seqs): \n",
        "    results[i,seq] = 1    # 특정 인덱스 위치를 1로 만듬 \n",
        "  return results\n",
        "\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)\n",
        "\n",
        "print(x_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QN0C4wjgld6Q",
        "outputId": "37a79303-e27a-4a94-b7ed-95a1f41067db"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 1. 1. ... 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')"
      ],
      "metadata": {
        "id": "kqa5qjOSqV8j"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 신경망 모델 만들기 \n",
        "- 입력데이터가 벡터, 레이블은 스칼라(0 or 1)\n",
        "- 이런 문제에 잘 작동하는 네트워크 종류는 relu 활성화함수 사용한 완전연결층 Dense(16,activation='relu') 을 그냥 쌓은것. \n",
        "- 매개변수 16은 은닉유닛(hidden unit)의 갯수 -> 가중치 행렬 W의 크기가 (input_dimension, 16) -> 입력데이터와 W를 점곱하면 입력데이터가 16차원으로 표현된 공간으로 투영 \n",
        "- 표현공간의 차원 -> 신경망이 내재된 표현을 학습할 때 가질 수 있는 자유도\n",
        "\n",
        "- 은닉유닛을 늘리면(표현공간을 더 고차원으로) 신경망이 더 복잡한 표현 학습 가능.-> 계산비용이 커지고 원치않는 패턴을 학습할 수도 있음.\n",
        "(훈련데이터에서는 성능향상되나, 테스트 데이터에서는 그렇지 않은 패턴)\n",
        "-Dense층을 쌓을 때 두가지 중요한 구조상 결정 필요\n",
        "  - how many layers? \n",
        "  - how much hidden unit for each layer? "
      ],
      "metadata": {
        "id": "hCPFgJ1Sqp3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation = 'relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(16, activation = 'relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "x-FT8qt7sEqb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 손실함수와 옵티마이저 선택 \n",
        "- 이진분류 문제 + 신경말 출력(->확률) : 마지막에 시그모이드 활성화함수를 사용한 하나의 유닛으로 된 층 사용 \n",
        "- binary crossentropy 손실함수가 적합 \n",
        "- MSE도 사용가능 \n",
        "- 확률을 출력하는 모델: 크로스엔트로피가 최선의 선택 \n",
        "\n",
        "-rmsprop 옵티마이저와 binary_crossentropy 손실함수 모델을 설정 \n",
        "-훈련하는 동안 accuracy 사용해 모니터링."
      ],
      "metadata": {
        "id": "NW6QJc79srMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import optimizers\n",
        "from tensorflow.keras import optimizers\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmXANoqPtUKU",
        "outputId": "f579c75d-78c8-40d2-f8f2-2bdee8676a03"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import losses\n",
        "from keras import metrics\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss=losses.binary_crossentropy, metrics=[metrics.binary_accuracy])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1sfxwh6wyqA",
        "outputId": "239cb8cc-c779-4c0f-aa1e-15d737264c65"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train validation (훈련 검증)\n",
        "- 훈련하는 동안 처음 본 데이터에 대한 모델의 정확도 측정\n",
        "- 원본 훈련 데이터에서 10000개의 샘플을 떼어 검증셋을 만듬 "
      ],
      "metadata": {
        "id": "AvElW4wP06gy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_val = x_train[:10000]\n",
        "partial_x_train = x_train[10000:]\n",
        "y_val = y_train[:10000]\n",
        "partial_y_train = y_train[:10000]"
      ],
      "metadata": {
        "id": "wTO7Mxnc0kdE"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 모델을 512개의 샘플씩 미니 batch를 만들어 20번의 epoch 동안 훈련 \n",
        "(x_train과 t_train 텐서에 있는 모든 샘플에 대해 20번 반복) \n",
        "- 동시에 따로 떼어놓은 1만개 샘플에서 손실과 정확도 측정"
      ],
      "metadata": {
        "id": "IW94EZNV6AKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model.fit(partial_x_train, partial_y_train, epochs=50, batch_size=512, validation_data=(x_val,y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "rSX7jbYT7RTf",
        "outputId": "135c2897-d20f-414c-80d5-0a7236e2a95c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-30985ed26c74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rmsprop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial_x_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartial_y_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1651\u001b[0m                            for i in tf.nest.flatten(single_data)))\n\u001b[1;32m   1652\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1653\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 15000\n  y sizes: 10000\nMake sure all arrays contain the same number of samples."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys()"
      ],
      "metadata": {
        "id": "5zLDWOVMBnAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "history_dict = history.history\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(loss)+1)\n",
        "\n",
        "plt.figure(figsize=(16,12))\n",
        "plt.plot(epochs, loss, 'bo', label ='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and Validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "1vcurQ6vBvzA",
        "outputId": "b1028116-d732-42ea-e374-3150bbb72208"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-8e30bf601b99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.clf()\n",
        "acc = history_dict['acc']\n",
        "val_acc = history_dict['val_acc']\n",
        "\n",
        "plt.figure(figsize=(16,12))\n",
        "plt.plot(epochs, acc, 'bo', label ='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and Validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O3xS2yjwCfkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 훈련손실이 에포크마다 감소, 정확도는 에포크마다 증가 \n",
        "- 경사하강법 최적화를 사용했을 때 반복마다 최소화되는 것이 손실이므로 기대와 같다. \n",
        "- 검증손실과 검증정확도는 훈련손실/훈련정확도와 다르다. \n",
        "- 4번째 에포크에서 역전 \n",
        "\n",
        "- 훈련셋에서 잘 작동하는 모델이 처음보는 데이터에서 자 작동하지 않음 -> Overfitting \n",
        "- 훈련데이터에 대한 성능 향상될수록 신경망이 overfitted -> 항상 훈련셋 이외 데이터에서 성능을 모니터링 해야함. "
      ],
      "metadata": {
        "id": "Gn7eJcy0DE6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# overfitting 방지를 위해 세번째 에포크 이후 훈련 중지 \n",
        "model = models.Sequential()   # 은닉층\n",
        "model.add(layers.Dense(16,activation='relu', input_shape=(10000,)))   #은닉층\n",
        "model.add(layers.Dense(16,activation='relu'))    #은닉층\n",
        "model.add(layers.Dense(1,activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
        "results= model.evaluate(x_test, y_test)\n",
        "print(results)"
      ],
      "metadata": {
        "id": "-ac63_NjDxCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련된 모델로 새로운 데이터에 대해 예측\n",
        "model.predict(x_test)"
      ],
      "metadata": {
        "id": "x_-DWkMIElSW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}