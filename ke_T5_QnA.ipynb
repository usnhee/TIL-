{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ke-T5_final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "background_execution": "on",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/usnhee/TIL-/blob/master/ke_T5_QnA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Runtime -> Change Runtime -> GPU"
      ],
      "metadata": {
        "id": "cKI3XnG963iC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "metadata": {
        "id": "A7xsHjk-tWvp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9a5eb91-2f2d-444a-e8cc-77fda79ef1a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.8.1+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torch-1.8.1%2Bcu111-cp37-cp37m-linux_x86_64.whl (1982.2 MB)\n",
            "\u001b[K     |█████████████▌                  | 834.1 MB 1.3 MB/s eta 0:14:24tcmalloc: large alloc 1147494400 bytes == 0x55b1de1dc000 @  0x7fe390ee4615 0x55b1a5a8b17c 0x55b1a5b6b47a 0x55b1a5a8df9d 0x55b1a5b7fd4d 0x55b1a5b01ec8 0x55b1a5afca2e 0x55b1a5a8f88a 0x55b1a5b01d30 0x55b1a5afca2e 0x55b1a5a8f88a 0x55b1a5afe719 0x55b1a5b80b76 0x55b1a5afdd95 0x55b1a5b80b76 0x55b1a5afdd95 0x55b1a5b80b76 0x55b1a5afdd95 0x55b1a5a8fce9 0x55b1a5ad3579 0x55b1a5a8e902 0x55b1a5b01c4d 0x55b1a5afca2e 0x55b1a5a8f88a 0x55b1a5afe719 0x55b1a5afca2e 0x55b1a5a8f88a 0x55b1a5afd8f6 0x55b1a5a8f7aa 0x55b1a5afdb4f 0x55b1a5afca2e\n",
            "\u001b[K     |█████████████████               | 1055.7 MB 61.6 MB/s eta 0:00:16tcmalloc: large alloc 1434370048 bytes == 0x55b222832000 @  0x7fe390ee4615 0x55b1a5a8b17c 0x55b1a5b6b47a 0x55b1a5a8df9d 0x55b1a5b7fd4d 0x55b1a5b01ec8 0x55b1a5afca2e 0x55b1a5a8f88a 0x55b1a5b01d30 0x55b1a5afca2e 0x55b1a5a8f88a 0x55b1a5afe719 0x55b1a5b80b76 0x55b1a5afdd95 0x55b1a5b80b76 0x55b1a5afdd95 0x55b1a5b80b76 0x55b1a5afdd95 0x55b1a5a8fce9 0x55b1a5ad3579 0x55b1a5a8e902 0x55b1a5b01c4d 0x55b1a5afca2e 0x55b1a5a8f88a 0x55b1a5afe719 0x55b1a5afca2e 0x55b1a5a8f88a 0x55b1a5afd8f6 0x55b1a5a8f7aa 0x55b1a5afdb4f 0x55b1a5afca2e\n",
            "\u001b[K     |█████████████████████▋          | 1336.2 MB 1.3 MB/s eta 0:08:02tcmalloc: large alloc 1792966656 bytes == 0x55b1a7664000 @  0x7fe390ee4615 0x55b1a5a8b17c 0x55b1a5b6b47a 0x55b1a5a8df9d 0x55b1a5b7fd4d 0x55b1a5b01ec8 0x55b1a5afca2e 0x55b1a5a8f88a 0x55b1a5b01d30 0x55b1a5afca2e 0x55b1a5a8f88a 0x55b1a5afe719 0x55b1a5b80b76 0x55b1a5afdd95 0x55b1a5b80b76 0x55b1a5afdd95 0x55b1a5b80b76 0x55b1a5afdd95 0x55b1a5a8fce9 0x55b1a5ad3579 0x55b1a5a8e902 0x55b1a5b01c4d 0x55b1a5afca2e 0x55b1a5a8f88a 0x55b1a5afe719 0x55b1a5afca2e 0x55b1a5a8f88a 0x55b1a5afd8f6 0x55b1a5a8f7aa 0x55b1a5afdb4f 0x55b1a5afca2e\n",
            "\u001b[K     |███████████████████████████▎    | 1691.1 MB 1.4 MB/s eta 0:03:34tcmalloc: large alloc 2241208320 bytes == 0x55b21244c000 @  0x7fe390ee4615 0x55b1a5a8b17c 0x55b1a5b6b47a 0x55b1a5a8df9d 0x55b1a5b7fd4d 0x55b1a5b01ec8 0x55b1a5afca2e 0x55b1a5a8f88a 0x55b1a5b01d30 0x55b1a5afca2e 0x55b1a5a8f88a 0x55b1a5afe719 0x55b1a5b80b76 0x55b1a5afdd95 0x55b1a5b80b76 0x55b1a5afdd95 0x55b1a5b80b76 0x55b1a5afdd95 0x55b1a5a8fce9 0x55b1a5ad3579 0x55b1a5a8e902 0x55b1a5b01c4d 0x55b1a5afca2e 0x55b1a5a8f88a 0x55b1a5afe719 0x55b1a5afca2e 0x55b1a5a8f88a 0x55b1a5afd8f6 0x55b1a5a8f7aa 0x55b1a5afdb4f 0x55b1a5afca2e\n",
            "\u001b[K     |████████████████████████████████| 1982.2 MB 1.3 MB/s eta 0:00:01tcmalloc: large alloc 1982177280 bytes == 0x55b297dae000 @  0x7fe390ee31e7 0x55b1a5ac1407 0x55b1a5a8b17c 0x55b1a5b6b47a 0x55b1a5a8df9d 0x55b1a5b7fd4d 0x55b1a5b01ec8 0x55b1a5afca2e 0x55b1a5a8f88a 0x55b1a5afdb4f 0x55b1a5afca2e 0x55b1a5a8f88a 0x55b1a5afdb4f 0x55b1a5afca2e 0x55b1a5a8f88a 0x55b1a5afdb4f 0x55b1a5afca2e 0x55b1a5a8f88a 0x55b1a5afdb4f 0x55b1a5afca2e 0x55b1a5a8f88a 0x55b1a5afdb4f 0x55b1a5a8f7aa 0x55b1a5afdb4f 0x55b1a5afca2e 0x55b1a5a8f88a 0x55b1a5afe719 0x55b1a5afca2e 0x55b1a5a8f88a 0x55b1a5afe719 0x55b1a5afca2e\n",
            "tcmalloc: large alloc 2477727744 bytes == 0x55b38248c000 @  0x7fe390ee4615 0x55b1a5a8b17c 0x55b1a5b6b47a 0x55b1a5a8df9d 0x55b1a5b7fd4d 0x55b1a5b01ec8 0x55b1a5afca2e 0x55b1a5a8f88a 0x55b1a5afdb4f 0x55b1a5afca2e 0x55b1a5a8f88a 0x55b1a5afdb4f 0x55b1a5afca2e 0x55b1a5a8f88a 0x55b1a5afdb4f 0x55b1a5afca2e 0x55b1a5a8f88a 0x55b1a5afdb4f 0x55b1a5afca2e 0x55b1a5a8f88a 0x55b1a5afdb4f 0x55b1a5a8f7aa 0x55b1a5afdb4f 0x55b1a5afca2e 0x55b1a5a8f88a 0x55b1a5afe719 0x55b1a5afca2e 0x55b1a5a8f88a 0x55b1a5afe719 0x55b1a5afca2e 0x55b1a5a8ff21\n",
            "\u001b[K     |████████████████████████████████| 1982.2 MB 2.8 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.9.1+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.9.1%2Bcu111-cp37-cp37m-linux_x86_64.whl (17.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.6 MB 1.8 MB/s \n",
            "\u001b[?25hCollecting torchaudio==0.8.1\n",
            "  Downloading torchaudio-0.8.1-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1+cu111) (4.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1+cu111) (1.21.6)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.9.1+cu111) (7.1.2)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet \"torchvision\" \"torchmetrics>=0.6\" \"pytorch-lightning>=1.4\" \"ipython[notebook]\" \"torch>=1.6, <1.9\""
      ],
      "metadata": {
        "id": "Z7yoH6stiSAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -V"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ozb4EkT4aDLY",
        "outputId": "db86d3a8-7dca-4d2b-e603-b01cd0c78051"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.__version__"
      ],
      "metadata": {
        "id": "GwYvXV66kKxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as pl\n",
        "print(pl.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ejA9jfcl7jD",
        "outputId": "34dbd03b-57b8-4baa-9df8-2cd061561543"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0zLl_aG6vJc"
      },
      "outputs": [],
      "source": [
        "# check for the GPU provided in the runtime\n",
        "!nvidia-smi\n",
        "# using quiet method for controlling the log\n",
        "# for suppressing the colored errors and warning in the terminal\n",
        "!pip install --quiet transformers==4.1.1\n",
        "# pytorch lightning for smoother model training and data loading\n",
        "!pip install --quiet https://github.com/PyTorchLightning/pytorch-lightning/releases/download/1.2.6/pytorch-lightning-1.2.6.tar.gz \n",
        "# using HuggingFace tokenizers\n",
        "!pip install --quiet tokenizers==0.9.4\n",
        "# Google's sentencepiece\n",
        "!pip install --quiet sentencepiece==0.1.94\n",
        "# mostly pl is used while doing complex model training\n",
        "import pytorch_lightning as pl\n",
        "print(pl.__version__)\n",
        "# argparse makes it easier to write user friendly command line interfaces\n",
        "import argparse\n",
        "# package for faster file name matching\n",
        "import glob\n",
        "# makiing directories for data \n",
        "import os\n",
        "# reading json files as the data is present in json files\n",
        "import json\n",
        "# time module for calculating the model runtime\n",
        "import time\n",
        "# Allows writing status messages to a file\n",
        "import logging\n",
        "# generate random float numbers uniformly\n",
        "import random\n",
        "# regex module for text \n",
        "import re\n",
        "# module provides various functions which work on \n",
        "# iterators too produce complex iterators\n",
        "from itertools import chain\n",
        "from string import punctuation\n",
        "# pandas for data manipulation\n",
        "import pandas as pd\n",
        "# numpy for array operations\n",
        "import numpy as np\n",
        "# PyTorch\n",
        "import torch\n",
        "# provides various classes representing file system paths\n",
        "# with appropriate semantics\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pytorch_lightning as pl\n",
        "# splitting the data \n",
        "from sklearn.model_selection import train_test_split\n",
        "# ANSII color formatting for ouput in terminal\n",
        "from termcolor import colored\n",
        "# wrapping paragraphs into string\n",
        "import textwrap\n",
        "# model checkpoints in pretrained model\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "'''\n",
        "optimizer - AdamW\n",
        "T5 Conditional Generator in which we'll give conditions\n",
        "T5 tokenizer because it is fast\n",
        "training the model without a learning rate\n",
        "'''\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "# Seeds all the processes including numpy torch and other imported modules.\n",
        "pl.seed_everything(0)\n",
        "# check the version provided by Lightning\n",
        "import pytorch_lightning as pl\n",
        "print(pl.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RuntimeError: CUDA out of memory. Tried to allocate 134.00 MiB (GPU 0; 15.90 GiB total capacity; 14.35 GiB already allocated; 29.75 MiB free; 14.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation. See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
        "# 이런 에러가 발생하는 이유는 batch size가 너무 크거나, 코드 상에서 메모리 누수가 발생했기 때문이라고 한다.\n",
        "# 출처 : https://beausty23.tistory.com/145\n",
        "\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "nIUPbUbORVrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading the Dataset"
      ],
      "metadata": {
        "id": "cUvcpoeH65ZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "from pathlib import Path\n",
        "from transformers import BertTokenizerFast\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "DG8WDlv1o81I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/6조_파이널PJT/data/ko1.0_add_ko2.0.csv')"
      ],
      "metadata": {
        "id": "zxmfGc_ooijH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to Create Pandas Dataframes of Questions and Answers\n",
        "This function will help us read the data from the folder containing multiple JSON files and read them to a dataframe to run manipulation on it and proceed further. "
      ],
      "metadata": {
        "id": "1etHB3Me6-bm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping all the rows with repeated context and questions pairs.\n",
        "# df = df.drop_duplicates(subset=[\"context\"]).reset_index(drop=True)\n",
        "df.shape\n",
        "len(df.question.unique())\n",
        "len(df.context.unique())\n",
        "sample_question = df.iloc[243]\n",
        "sample_question\n",
        "# Using textcolor to visualize the answer within the context\n",
        "def color_answer(question):\n",
        "  answer_start, answer_end = question[\"answer_start\"],question[\"answer_end\"]\n",
        "  context = question['context']\n",
        "  return  colored(context[:answer_start], \"white\") + \\\n",
        "    colored(context[answer_start:answer_end + 1], \"green\") + \\\n",
        "    colored(context[answer_end+1:], \"white\")\n",
        "print(sample_question['question'])\n",
        "print()\n",
        "print(\"Answer: \")\n",
        "for wrap in textwrap.wrap(color_answer(sample_question), width = 100):\n",
        "  print(wrap)"
      ],
      "metadata": {
        "id": "sosg-bHj608t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceea0f08-dd3c-473a-b6eb-6b881a31463b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "악티늄족을 핵 연료로 이용할 때 사용해야 하는 방사능 차단 물질은 어떤 종류인가?\n",
            "\n",
            "Answer: \n",
            "\u001b[37m악티늄족을 핵 연료로 이용할 때는, 방사능원을 차단하거나 방사성 물질을 막을 수 있는 \u001b[0m\u001b[32m고급 광물(\u001b[0m\u001b[37m예로 들자면 큰 잠재적 이득이 있는 자체\n",
            "발광 결정 등)을 사용해야 한다. 그러나, 악티늄족의 강한 방사능독으로 인해 자연에서의 이동과 악티늄족에 깊은 우려가 있기도 하다. 혼합산화물핵연료(MOX)에서 화학적으로 불안정한\n",
            "형태를 가진 악티늄족의 사용과 방사능 원천을 차단하는 것으로는 현대 안전 기준에는 적절하지 않다. 현재 안정되고 내구성이 좋은 악티늄족과 관련된 물질을 제작하는 것에 도전하고\n",
            "있다. 이것은 안전한 창고에 들어있어야 하고, 사용하고 나서 최종 처분할 수 있어야 한다. 악티늄족 고체 용액의 적용의 열쇠는 바로 내구성이 좋은 결정체의 상이다.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization\n",
        "In the following cells, we have instantiated the model and called its tokenizer. T5 tokenizer is pretty fast as compared to other BERT type tokenizers. We will run a sample of this on the text given below and do the decoding."
      ],
      "metadata": {
        "id": "DaPjX8817C0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using the base T5 model having 222M params\n",
        "MODEL_NAME ='KETI-AIR/ke-t5-base' \n",
        "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
        "sample_encoding = tokenizer('is the glass half empty or half full?', 'It depends on the initial state of the glass. If the glass starts out empty and liquid is added until it is half full, it is half full. If the glass starts out full and liquid is removed until it is half empty, it is half empty.')\n",
        "sample_encoding.keys()\n",
        "print(sample_encoding[\"input_ids\"])\n",
        "print(sample_encoding[\"attention_mask\"])\n",
        "print(len(sample_encoding['input_ids']), len(sample_encoding['attention_mask']))\n",
        "# Checking the decoding of the input ids\n",
        "preds = [\n",
        "        tokenizer.decode(input_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "        for input_id in sample_encoding['input_ids']\n",
        "]\n",
        "preds= \" \".join(preds)\n",
        "for wrap in textwrap.wrap(preds, width = 80):\n",
        "  print(wrap)\n",
        "encoding = tokenizer(\n",
        "    sample_question['question'],\n",
        "    sample_question['context'],\n",
        "    max_length=396,\n",
        "    padding='max_length',\n",
        "    truncation=\"only_second\",\n",
        "    return_attention_mask=True,\n",
        "    add_special_tokens=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "encoding.keys()\n",
        "tokenizer.special_tokens_map\n",
        "tokenizer.eos_token, tokenizer.eos_token_id\n",
        "tokenizer.decode(encoding['input_ids'].squeeze()) "
      ],
      "metadata": {
        "id": "EyDH6lwI606g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "953b79e9-06ec-4a61-f33d-2c2293524e6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[43, 5, 12857, 1648, 19876, 117, 1648, 1479, 85, 1, 223, 26000, 46, 5, 9113, 604, 14, 5, 12857, 3, 799, 5, 12857, 9162, 176, 19876, 13, 24483, 43, 1586, 1429, 60, 43, 1648, 1479, 4, 60, 43, 1648, 1479, 3, 799, 5, 12857, 9162, 176, 1479, 13, 24483, 43, 11514, 1429, 60, 43, 1648, 19876, 4, 60, 43, 1648, 19876, 3, 1]\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "63 63\n",
            "is the glass half empty or half full ? </s> It depends on the initial state of\n",
            "the glass . If the glass starts out empty and liquid is added until it is half\n",
            "full , it is half full . If the glass starts out full and liquid is removed\n",
            "until it is half empty , it is half empty . </s>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'악티늄족을 핵 연료로 이용할 때 사용해야 하는 방사능 차단 물질은 어떤 종류인가?</s> 악티늄족을 핵 연료로 이용할 때는, 방사능원을 차단하거나 방사성 물질을 막을 수 있는 고급 광물(예로 들자면 큰 잠재적 이득이 있는 자체 발광 결정 등)을 사용해야 한다. 그러나, 악티늄족의 강한 방사능독으로 인해 자연에서의 이동과 악티늄족에 깊은 우려가 있기도 하다. 혼합산화물핵연료(MOX)에서 화학적으로 불안정한 형태를 가진 악티늄족의 사용과 방사능 원천을 차단하는 것으로는 현대 안전 기준에는 적절하지 않다. 현재 안정되고 내구성이 좋은 악티늄족과 관련된 물질을 제작하는 것에 도전하고 있다. 이것은 안전한 창고에 들어있어야 하고, 사용하고 나서 최종 처분할 수 있어야 한다. 악티늄족 고체 용액의 적용의 열쇠는 바로 내구성이 좋은 결정체의 상이다.</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating labels for the answers\n",
        "In the following cells, We have to create necessary labels for the answers. This is required so that we can extract answers accordingly to the questions."
      ],
      "metadata": {
        "id": "g0pwP3Ij7Kc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer_encoding = tokenizer(\n",
        "    sample_question['answer_text'],\n",
        "    max_length=32,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    return_attention_mask=True,\n",
        "    add_special_tokens=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "tokenizer.decode(answer_encoding['input_ids'].squeeze())\n",
        "labels = answer_encoding[\"input_ids\"]\n",
        "labels\n",
        "labels[labels == 0] = -100\n",
        "labels "
      ],
      "metadata": {
        "id": "O4Ecg1Mn604W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc3f896d-9280-4c80-ae94-d44d37ab9117"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 4928, 42481,     1,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create dataset\n",
        "In the following cells, we have created the dataset for input in the model. This required setting the lengths, padding and tokenizer. The data has been taken for a BioQA dataset which is specifically for this task. "
      ],
      "metadata": {
        "id": "xXpr9q6j7OeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BioQADataset(Dataset):\n",
        "  def __init__(\n",
        "      self,\n",
        "      data:pd.DataFrame,\n",
        "      tokenizer:T5Tokenizer,\n",
        "      source_max_token_len: int = 396,\n",
        "      target_max_token_len: int = 32,\n",
        "      ):\n",
        "    self.data =  data\n",
        "    self.tokenizer =  tokenizer\n",
        "    self.source_max_token_len =  source_max_token_len\n",
        "    self.target_max_token_len =  target_max_token_len\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "  def __getitem__(self, index: int):\n",
        "    data_row = self.data.iloc[index]\n",
        "    source_encoding = tokenizer(\n",
        "      data_row['question'],\n",
        "      data_row['context'],\n",
        "      max_length=self.source_max_token_len,\n",
        "      padding='max_length',\n",
        "      truncation=\"only_second\",\n",
        "      return_attention_mask=True,\n",
        "      add_special_tokens=True,\n",
        "      return_tensors=\"pt\"\n",
        "      )\n",
        "    target_encoding = tokenizer(\n",
        "      data_row['answer_text'],\n",
        "      max_length=self.target_max_token_len,\n",
        "      padding='max_length',\n",
        "      truncation=True,\n",
        "      return_attention_mask=True,\n",
        "      add_special_tokens=True,\n",
        "      return_tensors=\"pt\"\n",
        "      )\n",
        "    labels = target_encoding['input_ids']\n",
        "    labels[labels==0] = -100\n",
        "    return dict(\n",
        "        question=data_row['question'],\n",
        "        context=data_row['context'],\n",
        "        answer_text=data_row['answer_text'],\n",
        "        input_ids=source_encoding[\"input_ids\"].flatten(),\n",
        "        attention_mask=source_encoding['attention_mask'].flatten(),\n",
        "        labels=labels.flatten()\n",
        "    )\n",
        "sample_dataset = BioQADataset(df, tokenizer)\n",
        "for data in sample_dataset:\n",
        "  print(\"Question: \", data['question'])\n",
        "  print(\"Answer text: \", data['answer_text'])\n",
        "  print(\"Input_ids: \", data['input_ids'][:10])\n",
        "  print(\"Labels: \", data['labels'][:10])\n",
        "  break "
      ],
      "metadata": {
        "id": "sJlosUyi602C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26963dff-d71b-4cd4-88b2-d991b2134a5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:  바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가?\n",
            "Answer text:  교향곡\n",
            "Input_ids:  tensor([  396,   775, 21778, 63122,     6,  1649,  8643, 15171, 12022,  6937])\n",
            "Labels:  tensor([58337,     1,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split Dataset into Train and Test\n",
        "In the following cells, we have split the data into two parts. Test size is small due to the heavy model needed large data for training."
      ],
      "metadata": {
        "id": "XnpLllfZ7UAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, val_df = train_test_split(df, test_size=0.05)\n",
        "train_df.shape,  val_df.shape "
      ],
      "metadata": {
        "id": "ygEscQ_m60z7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d5f4def-2ba4-4d11-e94f-f810652bdaaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((96849, 5), (5098, 5))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Pytorch Lightning Module\n",
        "In the following cells, we have created the lightning module. This is made when we have complex models. Hence Pytorch has introduced this to smoothen the process."
      ],
      "metadata": {
        "id": "xaQEOKYU7bO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " class BioDataModule(pl.LightningDataModule):\n",
        "   def __init__(\n",
        "       self,\n",
        "       train_df: pd.DataFrame,\n",
        "       test_df: pd.DataFrame,\n",
        "       tokenizer:T5Tokenizer,\n",
        "       batch_size: int = 8,\n",
        "       source_max_token_len: int = 396,\n",
        "       target_max_token_len: int = 32,\n",
        "       ):\n",
        "     super().__init__()\n",
        "     self.train_df = train_df\n",
        "     self.test_df = test_df\n",
        "     self.tokenizer = tokenizer\n",
        "     self.batch_size = batch_size\n",
        "     self.source_max_token_len = source_max_token_len\n",
        "     self.target_max_token_len = target_max_token_len\n",
        "   def setup(self):\n",
        "     self.train_dataset = BioQADataset(\n",
        "         self.train_df,\n",
        "         self.tokenizer,\n",
        "         self.source_max_token_len,\n",
        "         self.target_max_token_len\n",
        "         )\n",
        "     self.test_dataset = BioQADataset(\n",
        "     self.test_df,\n",
        "     self.tokenizer,\n",
        "     self.source_max_token_len,\n",
        "     self.target_max_token_len\n",
        "     )\n",
        "   def train_dataloader(self):\n",
        "     return DataLoader(\n",
        "         self.train_dataset,\n",
        "         batch_size=self.batch_size,\n",
        "         shuffle=True,\n",
        "         num_workers=4\n",
        "         )\n",
        "   def val_dataloader(self):\n",
        "     return DataLoader(\n",
        "         self.test_dataset,\n",
        "         batch_size=self.batch_size,\n",
        "         num_workers=4\n",
        "         )\n",
        "   def test_dataloader(self):\n",
        "     return DataLoader(\n",
        "         self.test_dataset,\n",
        "         batch_size=1,\n",
        "         num_workers=4\n",
        "         )\n",
        " BATCH_SIZE = 4\n",
        " N_EPOCHS = 6\n",
        " data_module = BioDataModule(train_df, val_df, tokenizer, batch_size=BATCH_SIZE)\n",
        " data_module.setup() "
      ],
      "metadata": {
        "id": "7lbxwD9060x4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the PyTorch lightning module using T5ForConditionalGeneration model\n",
        "In the following cells, we have leveraged a T5 Conditional Generator which will produce text based on some condition (this is going to be our application task – QA)."
      ],
      "metadata": {
        "id": "EQ_pI2j-7wZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BioQAModel(pl.LightningModule):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, return_dict=True)\n",
        "  def forward(self, input_ids, attention_mask, labels=None):\n",
        "    output = self.model(\n",
        "        input_ids, \n",
        "        attention_mask=attention_mask,\n",
        "        labels=labels)\n",
        "    return output.loss, output.logits\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    input_ids = batch['input_ids']\n",
        "    attention_mask=batch['attention_mask']\n",
        "    labels = batch['labels']\n",
        "    loss, outputs = self(input_ids, attention_mask, labels)\n",
        "    self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
        "    return {\"loss\": loss, \"predictions\":outputs, \"labels\": labels}\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    input_ids = batch['input_ids']\n",
        "    attention_mask=batch['attention_mask']\n",
        "    labels = batch['labels']\n",
        "    loss, outputs = self(input_ids, attention_mask, labels)\n",
        "    self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
        "    return loss\n",
        "  def test_step(self, batch, batch_idx):\n",
        "    input_ids = batch['input_ids']\n",
        "    attention_mask=batch['attention_mask']\n",
        "    labels = batch['labels']\n",
        "    loss, outputs = self(input_ids, attention_mask, labels)\n",
        "    self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
        "    return loss\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = AdamW(self.parameters(), lr=0.0001)\n",
        "    return optimizer\n",
        "model = BioQAModel()"
      ],
      "metadata": {
        "id": "iBcI-dJq60td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Trainer from PyTorch-Lightning to Finetune Model Using our Dataset\n",
        "We have used a trainer module to test the model and fine-tune it in the following cells. This is helpful when the model is huge, and you want to get a warmup of the model without changing the learning rate. "
      ],
      "metadata": {
        "id": "NRtV7DP-725H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BioQAModel()\n",
        "# To record the best performing model using checkpoint\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath=\"/content/drive/MyDrive/final_pjt/ET5/model\",\n",
        "    filename=\"best-checkpoint-test\",\n",
        "    save_top_k=1,\n",
        "    verbose=True,\n",
        "    monitor=\"val_loss\",\n",
        "    mode=\"min\"\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    checkpoint_callback=checkpoint_callback,\n",
        "    max_epochs=N_EPOCHS,\n",
        "    gpus=1,\n",
        "    progress_bar_refresh_rate = 30\n",
        ") "
      ],
      "metadata": {
        "id": "YbY2efAh60rG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be36f53b-90c1-461f-fd7a-386d32715a5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:152: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x7f79085cb390>)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x7f79085cb390>)`.\n",
            "  # remove all callbacks with a type that occurs in model callbacks\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:97: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=30)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
            "  refresh_rate = 1 if refresh_rate is None else refresh_rate\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Tensorboard\n",
        "In the following cells, we have used a Tensorboard for seeing how the model is progressing against time and epochs. This is very helpful when running huge models. "
      ],
      "metadata": {
        "id": "zPhiHkXq76UI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.fit(model, data_module)\n",
        "trainer.test()"
      ],
      "metadata": {
        "id": "3NrlOviV76GC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "outputId": "c0795653-9877-4e61-c411-f5b285146aa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: unrecognized option '--rf'\n",
            "Try 'rm --help' for more information.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-01d7ff804f72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# %tensorboard --logdir ./lightning_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rm --rf lightning_logs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# evaluate the model according to the last checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mResult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    770\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_metrics_to_cpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m                     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;31m# hook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_evaluation_batch_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m                 \u001b[0;31m# lightning module methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_skip_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m         \u001b[0;31m# ref model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m__set_ckpt_path\u001b[0;34m(self, ckpt_path, model_provided, model_connected)\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_lightning.callbacks.fault_tolerance'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predictions"
      ],
      "metadata": {
        "id": "TMMTQHrf7_7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model = BioQAModel.load_from_checkpoint(\"/content/drive/MyDrive/final_pjt/ET5/model/best-checkpoint-large-v1.ckpt\")\n",
        "# trained_model = BioQAModel.load_from_checkpoint('/content/drive/MyDrive/final_pjt/ET5/model/best-checkpoint.ckpt')\n",
        "trained_model.freeze() # "
      ],
      "metadata": {
        "id": "OCWi4Qwh76Df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Answers for the Questions in the Validation Set\n",
        "In the following cells, we have tried and tested some questions whose answers were saved with us in the valid dataset against the predicted values given by the model. As you’ll notice the T5 model is so powerful that it will predict exact values."
      ],
      "metadata": {
        "id": "DAV2X1Er8DzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(question):\n",
        "  source_encoding=tokenizer(\n",
        "      question['question'],\n",
        "      question['context'],\n",
        "      max_length = 396,\n",
        "      padding=\"max_length\",\n",
        "      truncation=\"only_second\",\n",
        "      return_attention_mask=True,\n",
        "      add_special_tokens=True,\n",
        "      return_tensors=\"pt\"\n",
        "  )\n",
        "  generated_ids = trained_model.model.generate(\n",
        "      input_ids=source_encoding[\"input_ids\"],\n",
        "      attention_mask=source_encoding[\"attention_mask\"],\n",
        "      num_beams=1,  # greedy search\n",
        "      max_length=80,\n",
        "      repetition_penalty=2.5,\n",
        "      early_stopping=True,\n",
        "      use_cache=True)\n",
        "  preds = [\n",
        "          tokenizer.decode(generated_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "          for generated_id in generated_ids\n",
        "  ]\n",
        "  return \"\".join(preds)"
      ],
      "metadata": {
        "id": "7JO9kuX9L825"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample_question = val_df.iloc[20]\n",
        "# sample_question[\"question\"]\n",
        "# sample_question[\"answer_text\"]  # Label Answer\n",
        "# generate_answer(sample_question)  # Predicted answer\n",
        "sample_question = val_df.iloc[16]\n",
        "# sample_question[\"answer_text\"]\n",
        "generate_answer(sample_question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ca03eba0-2e64-4b5d-cba7-39897cecfe2d",
        "id": "qn61O06jL826"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1만장'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUNf_rSvrnFp"
      },
      "outputs": [],
      "source": [
        " !git clone https://github.com/monologg/KoBERT-Transformers.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/')"
      ],
      "metadata": {
        "id": "M_D_owm4rvMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tokenization_kobert"
      ],
      "metadata": {
        "id": "Pn92fjMCr2zT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenization_kobert import KoBertTokenizer"
      ],
      "metadata": {
        "id": "iPZlfRlyr3R3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_t = KoBertTokenizer.from_pretrained('monologg/kobert')"
      ],
      "metadata": {
        "id": "Zj4M0ua5r96U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow #텐서플로우 설치\n",
        "!pip install sklearn #사이킷런 설치\n",
        "!pip install konlpy"
      ],
      "metadata": {
        "id": "peVjkD1hsCSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics.pairwise import manhattan_distances\n",
        "manhattan_d = manhattan_distances(tfidf_normalized[0:1],tfidf_normalized[1:2])"
      ],
      "metadata": {
        "id": "FSoLBiUUsP2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosin_sim(t_list, question):\n",
        "  t_list['score']=0.0\n",
        "  t_list.reset_index(drop=True, inplace=True)\n",
        "  for i in range(len(t_list)):\n",
        "      sentences = (t_list['context'][i], question)\n",
        "      tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n",
        "      cos_similar = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
        "      t_list['score'][i] = cos_similar[0][0]\n",
        "  # return cos_similar[0][0], type(cos_similar[0][0])\n",
        "  return t_list.loc[t_list['score'].idxmax()]['context']"
      ],
      "metadata": {
        "id": "cgf-IKO0sQfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cos_sim(A, B):\n",
        "  return dot(A, B)/(norm(A)*norm(B))"
      ],
      "metadata": {
        "id": "SXzsjQuEsS5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def in_title(question, dat):\n",
        "  dat = dat.drop_duplicates(['context'], ignore_index = True)\n",
        "  t_list = []\n",
        "  con_list = []\n",
        "  for i in range(len(dat)):\n",
        "    if dat['title'][i].split('(')[0] in question.replace(' ', ''):  \n",
        "      contet = dat.loc[i,:]\n",
        "      t_list.append(contet)\n",
        "\n",
        "    else:\n",
        "      pass\n",
        "  t_list = pd.DataFrame(t_list, columns = dat.columns)\n",
        "  con_list = dat[dat['context'].str.contains(okt.nouns(question)[0])]\n",
        "  use = pd.concat([t_list, con_list], ignore_index=True)\n",
        "  if len(use) >0:\n",
        "    return use\n",
        "  else :\n",
        "    return print('데이터 없음')\n"
      ],
      "metadata": {
        "id": "e3HNkiQFstBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def title_re(t_list, question):     \n",
        "\n",
        "  t_list['score']=0.0\n",
        "  t_list.reset_index(drop=True, inplace=True)\n",
        "  for i in range(len(t_list)):\n",
        "      sentences = (t_list['question'][i], question)\n",
        "      tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n",
        "      tfidf_normalized = tfidf_matrix/np.sum(tfidf_matrix)\n",
        "      manhattan_d = manhattan_distances(tfidf_normalized[0:1],tfidf_normalized[1:2])\n",
        "      t_list['score'][i] = manhattan_d[0][0]\n",
        "  t_list = t_list[t_list['score'] == min(t_list['score'])]\n",
        "  return t_list\n"
      ],
      "metadata": {
        "id": "abSgAK-JstMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def anw_test(context, question):\n",
        "\n",
        "  source_encoding=tokenizer(\n",
        "    question,\n",
        "    context,\n",
        "    max_length = 396,\n",
        "    padding=\"max_length\",\n",
        "    truncation=\"only_second\",\n",
        "    return_attention_mask=True,\n",
        "    add_special_tokens=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "  generated_ids = trained_model.model.generate(\n",
        "    input_ids=source_encoding[\"input_ids\"],\n",
        "    attention_mask=source_encoding[\"attention_mask\"],\n",
        "    num_beams=1,  # greedy search\n",
        "    max_length=80,\n",
        "    repetition_penalty=2.5,\n",
        "    early_stopping=True,\n",
        "    use_cache=True)\n",
        "  preds = [\n",
        "        tokenizer.decode(generated_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "        for generated_id in generated_ids]\n",
        "  return \"\".join(preds), context "
      ],
      "metadata": {
        "id": "6gk-IWFcstPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def last(question, dat):\n",
        "  dat = dat.drop_duplicates(['context'], ignore_index = True)\n",
        "  a = in_title(question,dat)   ## 타이틀, 컨텍스트 \n",
        "  b= title_re(a, question)    ## 질문,타이틀 유사도 \n",
        "  context =cosin_sim(b, question)    \n",
        "  answer = anw_test(context, question)   ## 정답 예측, 컨텍스트 \n",
        "  return answer"
      ],
      "metadata": {
        "id": "FScixs5WtKxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 예측 정답과 실제 정답 비교용 데이터프레임 생성 \n"
      ],
      "metadata": {
        "id": "-Qivhar3tqvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_df = dat.sample(n = 100 , random_state=15)\n",
        "re_sample = sample_df.reset_index()\n",
        "ans_list = []\n",
        "context_list = []\n",
        "\n",
        "for i in range(100): \n",
        "  try:\n",
        "    answer = last(re_sample.loc[i,'question'], re_sample)\n",
        "    ans_list.append(answer)\n",
        "    # for i in answer: \n",
        "    #   ans_list.append(a)\n",
        "    #   i+=1\n",
        "    #   context_list.append(a)\n",
        "  except:\n",
        "    ans_list.append('NaN')\n",
        "    context_list.append('NaN')"
      ],
      "metadata": {
        "id": "zXV6Cq2estRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df = re_sample.copy()\n",
        "for i in ans_list: \n",
        "  new_df['pred_ans'] = ans_list\n",
        "\n",
        "new_df.drop(['answer_start','answer_end'], axis=1)"
      ],
      "metadata": {
        "id": "5QtBoLhatZzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@misc{ke_t5,\n",
        "    author       = {KETI AIRC},\n",
        "    title        = {KE-T5: Korean English T5},\n",
        "    month        = may,\n",
        "    year         = 2022,\n",
        "    url          = {https://github.com/AIRC-KETI/ke-t5}\n",
        "}"
      ],
      "metadata": {
        "id": "SqnfOfDDq-LL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}